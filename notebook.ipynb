{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset from *JSONL* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Converting JSONL file into JSON\n",
    "data = dict()\n",
    "with open('../webis-touche2020/corpus.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for cnt, line in enumerate(f):\n",
    "        d = json.loads(line)\n",
    "        data[d['_id']] = d['title'] + ' ' + d['text']\n",
    "        \n",
    "docs = list(data.values())\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Processing dataset text based on a bunch of rules:\n",
    "###### Rule(1): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1): No whitespace!\n",
    "# 'goingto' 'playingPoker' 'movingpictures' 'memorizingtables'\n",
    "# So there are two cases: \n",
    "# 1- 'goingto'. So the program has to discover words that are grammatically correct but are not separated with whitespace.\n",
    "# 2- 'playingPoker' ..\n",
    "# \n",
    "# TODO (2): Hyphenated expressions!\n",
    "# Example: Should-we-deport-all-illegal-immigrants-we-find-in-the-US\n",
    "#\n",
    "# TODO (3): Spell checking!\n",
    "# Example: \"occurec\"\n",
    "#\n",
    "# TODO (4): Points and commas in between!\n",
    "# Example: men.Affirmative (Frequent)\n",
    "# \n",
    "# TODO (5): Handling Links!\n",
    "# Example: //www.commieblaster.com\n",
    "#          //sas-space.sas.ac.uk/2563/1/Amicus79_Ahmed\n",
    "#          \n",
    "# TODO (6): Handling abbreviations and acronyms\n",
    "# \n",
    "# TODO (7): Swearing words handling?!\n",
    "# \n",
    "# TODO (8): Handling Emojis!\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spellchecker import SpellChecker\n",
    "from emoji import emoji_list, is_emoji, demojize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "example = 'USA is a text with some abbreviations lik OOP and it is not expected of pythn language to detect US unknownwords because FBI is not here! But hey! This is UK stff! FASEB gun is great!'\n",
    "\n",
    "email_example = {\n",
    "    'doc1': \" Hi!! ?? ,,  *** ## //www.commieblaster.com yoyo ðŸª€ðŸª€\",\n",
    "    'doc2': \"hey this is me email   USA FAB ðŸ”—  //sas-space.sas.ac.uk/2563/1/Amicus79_Ahmed ant youu this iss misspeled\",\n",
    "    \"doc3\": \"ttiip://google.com ðŸ˜Ž  ðŸ˜‚ðŸ˜‚ yo!!!!\"\n",
    "}\n",
    "\n",
    "def spellCheck(tokens):\n",
    "    \"\"\"\n",
    "    Spell checker.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in misspelled:\n",
    "            corrected = spell.correction(token)\n",
    "            if corrected is not None:\n",
    "                tokens[i] = corrected\n",
    "    return tokens\n",
    "\n",
    "# TODO: Detect abbreviations in a given text \n",
    "def detect_abbreviations(text):\n",
    "    pattern = r'\\b[A-Z]{2,}\\b'\n",
    "    abbs = re.findall(pattern, text)\n",
    "    return abbs\n",
    "\n",
    "def detect_url(doc):\n",
    "    \"\"\"\n",
    "    Detects almost all kinds of different URLs\n",
    "    \n",
    "    Args:\n",
    "    - doc (str): A string to look for all the matches of the url pattern \n",
    "    \n",
    "    Returns:\n",
    "    - (list): List of matches. Returns None if no matches\n",
    "    \"\"\"\n",
    "    pattern = r'(?:https?\\:)?(?:\\/\\/)?(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z0-9]+(?:\\/[a-zA-z0-9]+)*'\n",
    "    return re.match(pattern, doc)\n",
    "        \n",
    "def wordnet_abbreviation_expand(abbreviation):\n",
    "    \"\"\"\n",
    "    Find abbreviations using WordNet.\n",
    "\n",
    "    Args:\n",
    "    - abbreviation (str): The abbreviation to search for in WordNet.\n",
    "\n",
    "    Returns:\n",
    "    - res (list): A list of strings that represent possible meanings of the word.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    synsets = wn.synsets(abbreviation)\n",
    "    for synset in synsets:\n",
    "        ext_abb = str(synset)[8:-7].replace('_',' ')\n",
    "        res.append(ext_abb)\n",
    "    return res\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# def custom_tokenizer(doc):\n",
    "#     punctuation_pattern = r'[!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]'\n",
    "#     filtered_tokens = []\n",
    "#     tokens = word_tokenize(doc)\n",
    "    \n",
    "#     for token in tokens:\n",
    "#         # URL handling\n",
    "#         if detect_url(token) is not None:\n",
    "#             filtered_tokens.append(\"Link\")\n",
    "#         # Emoji Handling\n",
    "#         elif emoji_list(token):\n",
    "#             normalized_emoji = demojize(token).replace(':', ' ').strip()\n",
    "#             filtered_tokens.append(normalized_emoji)\n",
    "#         else: \n",
    "#             # If token matches a punctuation mark; ignore it\n",
    "#             if re.match(punctuation_pattern, token) is None:\n",
    "#                 filtered_tokens.append(token)\n",
    "#     return filtered_tokens          \n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from emoji import emoji_list, is_emoji, demojize\n",
    "    \n",
    "    def detect_url(doc):\n",
    "        pattern = r'(?:https?\\:)?(?:\\/\\/)?(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z0-9]+(?:\\/[a-zA-z0-9]+)*'\n",
    "        re.match(pattern, doc)\n",
    "    \n",
    "    punctuation_pattern = r'[!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]'\n",
    "    filtered_tokens = []\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        # URL handling\n",
    "        if detect_url(token) is not None:\n",
    "            filtered_tokens.append(\"Link\")\n",
    "        # Emoji Handling\n",
    "        elif emoji_list(token):\n",
    "            normalized_emoji = demojize(token).replace(':', ' ').strip()\n",
    "            filtered_tokens.append(normalized_emoji)\n",
    "        else: \n",
    "            # If token matches a punctuation mark; ignore it\n",
    "            if re.match(punctuation_pattern, token) is None:\n",
    "                filtered_tokens.append(token) \n",
    "    return filtered_tokens \n",
    "\n",
    "\n",
    "\n",
    "# TODO: Remove when done\n",
    "# with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "#     for doc in docs:\n",
    "#         for token in custom_tokenizer(doc):\n",
    "#             f.write(token + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining `TfidfVectorizer` model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Vectorizer \n",
    "vectorizer = TfidfVectorizer(      \n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=\"english\",\n",
    "    # use_idf=True, # default \n",
    "    lowercase=True,\n",
    "    # sublinear_tf=True, # default = False\n",
    "    # encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train `TfidfVectorizer` model on dataset and queries\n",
    "Then storing `tfidf_matrix` and the `vectorizer` instance as `pickle` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Query looks like:\n",
    "    \"query\": {\n",
    "        \"_id\": string, // Can be mapped to numbers\n",
    "        \"text\": string,\n",
    "        \"metadata\": {\n",
    "            \"description\": string,\n",
    "            \"narrative\": string,\n",
    "        },\n",
    "    }\n",
    "\"\"\"\n",
    "queries = dict()\n",
    "with open('../webis-touche2020/queries.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for cnt, line in enumerate(f):\n",
    "        query = json.loads(line)\n",
    "        query_metadata = query['metadata']['narrative']\n",
    "        queries[query['_id']] = query['text'] \n",
    "\n",
    "# Train model \n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# with open('feature_names.txt', 'w', encoding='utf-8') as fff:\n",
    "#     for feat_name in vectorizer.get_feature_names_out():\n",
    "#         fff.writelines(feat_name + ' ')\n",
    "\n",
    "# Writing tfidf matrix to a compressed binary file \n",
    "with open('matrix.pkl', 'wb') as mf:\n",
    "    dill.dump(tfidf_matrix, mf)\n",
    "mf.close()\n",
    "\n",
    "# Writing vectorizer 'model' to a compressed binary file\n",
    "with open('model.pkl', 'wb') as bf:\n",
    "    dill.dump(vectorizer, bf)\n",
    "bf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read `vectorizer` and `tfidf_matrix` from `pickle` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tfidf_matrix():\n",
    "    with open('matrix.pkl', 'rb') as mf:\n",
    "        tfidf_matrix = dill.load(mf)\n",
    "    mf.close()\n",
    "    return tfidf_matrix\n",
    "\n",
    "def get_vectorizer_instance() -> TfidfVectorizer:\n",
    "    with open('model.pkl', 'rb') as mf:\n",
    "        vectorizer = dill.load(mf)\n",
    "    mf.close()\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# tfidf_matrix = get_tfidf_matrix()\n",
    "# vectorizer = get_vectorizer_instance()\n",
    "\n",
    "def find_top_k_results(user_query, tfidf_matrix, docs, k=10): \n",
    "    # Transform user query into vector\n",
    "    user_query_vector = vectorizer.transform([user_query])\n",
    "\n",
    "    # Calculating cosine similarity between user query and stored data\n",
    "    similarites = cosine_similarity(user_query_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get indecies of top K elements\n",
    "    top_indices = np.argpartition(similarites, -k, axis=None)[-k:]\n",
    "\n",
    "    # Sort top K results by similarity score\n",
    "    top_indices_sorted = top_indices[np.argsort(similarites.ravel()[top_indices])]\n",
    "\n",
    "    # Get top K results\n",
    "    top_results = [(docs[i], keys[i], similarites[0, i]) for i in top_indices_sorted]\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "\n",
    "user_query = \"Should teachers get tenure?\"\n",
    "k = 10\n",
    "top_results = find_top_k_results(user_query, tfidf_matrix, docs, k)\n",
    "\n",
    "\n",
    "# Print\n",
    "with open('results.txt', 'w', encoding='utf-8') as rf:\n",
    "    rf.writelines\n",
    "    for doc, doc_id, similarity_score in top_results:\n",
    "        rf.writelines(f\"Doc Id: {doc_id}\\n\")\n",
    "        # rf.writelines(f\"Document: {doc}\\n\")\n",
    "        # rf.writelines(f\"Cosine Similarity Score: {similarity_score}\\n\")\n",
    "        # rf.writelines('--------------------------------------\\n')\n",
    "rf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "#### Calculating *precision* score and *recall* score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Precision: 0.7,  Recall: 0.1590909090909091, AP: 0.9129251700680271\n",
      "Mean Average Precision (mAP): 0.018631125919755655\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "qrels_file = '../webis-touche2020/qrels/test.tsv'\n",
    "\n",
    "def create_qrels_inverted(qrels_file):\n",
    "    inverted_index = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        next(f) # Skip first line of the file\n",
    "        for line in f:\n",
    "            query_id, document_id, score = line.strip().split('\\t')\n",
    "            score = int(score)\n",
    "            query_id = int(query_id)\n",
    "            if query_id not in inverted_index:\n",
    "                inverted_index[query_id] = []\n",
    "            inverted_index[query_id].append((document_id, score))\n",
    "                    \n",
    "    # Sort index in descending order\n",
    "    for query_id, doc_scores in inverted_index.items():\n",
    "        inverted_index[query_id] = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return inverted_index\n",
    "\n",
    "def get_relevant_docs_for_query_i(query_id, k=10):\n",
    "    relevant_docs = set()\n",
    "    # for query_id, doc_scores in zip(qrels.keys(), qrels.values()):\n",
    "    for doc_id, score in qrels[query_id]:\n",
    "        relevant_docs.add(doc_id)\n",
    "    return relevant_docs\n",
    "\n",
    "def calculate_average_precision(relevant_docs, retrieved_docs, k=10): \n",
    "    precision_sum = 0\n",
    "    true_positives_at_k = 0\n",
    "    for i, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if retrieved_doc in relevant_docs:\n",
    "            true_positives_at_k += 1\n",
    "            precision_sum += true_positives_at_k / i\n",
    "    return precision_sum / true_positives_at_k if true_positives_at_k > 0 else 0\n",
    "        \n",
    "def evaluate(top_results: List, query_id: str, k):\n",
    "    relevant_docs = get_relevant_docs_for_query_i(query_id)\n",
    "    retrieved_docs = set(result[1] for result in top_results)\n",
    "   \n",
    "    true_positives = retrieved_docs.intersection(relevant_docs)\n",
    "    \n",
    "    # Precision\n",
    "    precision = len(true_positives) / k if k > 0 else 0\n",
    "    # Recall \n",
    "    recall = len(true_positives)/ len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "    # Average Precision\n",
    "    ap = calculate_average_precision(relevant_docs, retrieved_docs)\n",
    "    \n",
    "    return precision, recall, ap\n",
    "\n",
    "qrels = create_qrels_inverted(qrels_file)\n",
    "\n",
    "aps = []\n",
    "with open('evaluation.tsv', 'w', newline='', encoding='utf-8') as ef:  \n",
    "    writer = csv.writer(ef, delimiter='\\t')  \n",
    "    writer.writerow(['Query-id', 'Precision', 'Recall', 'AP'])\n",
    "    for query_id in qrels.keys():\n",
    "        p, r, ap = evaluate(top_results, query_id, k)\n",
    "        aps.append(ap)\n",
    "        writer.writerow([query_id, p, r, ap])\n",
    "        if p != 0:\n",
    "            print(f\"Query {query_id}: Precision: {p},  Recall: {r}, AP: {ap}\")\n",
    "ef.close()\n",
    "\n",
    "\n",
    "mAP = sum(aps) / len(aps)\n",
    "print(f\"Mean Average Precision (mAP): {mAP}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
