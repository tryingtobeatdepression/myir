{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset from *JSONL* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dill\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Converting JSONL file into JSON\n",
    "data = dict()\n",
    "with open('../webis-touche2020/corpus.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for cnt, line in enumerate(f):\n",
    "        d = json.loads(line)\n",
    "        data[d['_id']] = d['title'] + ' ' + d['text']\n",
    "        \n",
    "docs = list(data.values())\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Processing dataset text based on a bunch of rules:\n",
    "###### Rule(1): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1): No whitespace!\n",
    "# 'goingto' 'playingPoker' 'movingpictures' 'memorizingtables'\n",
    "# So there are two cases: \n",
    "# 1- 'goingto'. So the program has to discover words that are grammatically correct but are not separated with whitespace.\n",
    "# 2- 'playingPoker' ..\n",
    "# \n",
    "# TODO (2): Hyphenated expressions!\n",
    "# Example: Should-we-deport-all-illegal-immigrants-we-find-in-the-US\n",
    "#\n",
    "# TODO (3): Spell checking!\n",
    "# Example: \"occurec\"\n",
    "#\n",
    "# TODO (4): Points and commas in between!\n",
    "# Example: men.Affirmative (Frequent)\n",
    "# \n",
    "# TODO (5): Handling Links!\n",
    "# Example: //www.commieblaster.com\n",
    "#          //sas-space.sas.ac.uk/2563/1/Amicus79_Ahmed\n",
    "#          \n",
    "# TODO (6): Handling abbreviations and acronyms\n",
    "# \n",
    "# TODO (7): Swearing words handling?!\n",
    "# \n",
    "# TODO (8): Handling Emojis!\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spellchecker import SpellChecker\n",
    "from emoji import emoji_list, is_emoji, demojize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "example = 'USA is a text with some abbreviations lik OOP and it is not expected of pythn language to detect US unknownwords because FBI is not here! But hey! This is UK stff! FASEB gun is great!'\n",
    "\n",
    "email_example = {\n",
    "    'doc1': \" Hi!! ?? ,,  *** ## //www.commieblaster.com yoyo ðŸª€ðŸª€\",\n",
    "    'doc2': \"hey this is me email   USA FAB ðŸ”—  //sas-space.sas.ac.uk/2563/1/Amicus79_Ahmed ant youu this iss misspeled\",\n",
    "    \"doc3\": \"ttiip://google.com ðŸ˜Ž  ðŸ˜‚ðŸ˜‚ yo!!!!\"\n",
    "}\n",
    "\n",
    "# TODO: Detect abbreviations in a given text \n",
    "def detect_abbreviations(text):\n",
    "    pattern = r'\\b[A-Z]{2,}\\b'\n",
    "    abbs = re.findall(pattern, text)\n",
    "    return abbs\n",
    "\n",
    "def detect_url(doc):\n",
    "    \"\"\"\n",
    "    Detects almost all kinds of different URLs\n",
    "    \n",
    "    Args:\n",
    "    - doc (str): A string to look for all the matches of the url pattern \n",
    "    \n",
    "    Returns:\n",
    "    - (list): List of matches. Returns None if no matches\n",
    "    \"\"\"\n",
    "    pattern = r'(?:https?\\:)?(?:\\/\\/)?(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z0-9]+(?:\\/[a-zA-z0-9]+)*'\n",
    "    return re.match(pattern, doc)\n",
    "        \n",
    "def wordnet_abbreviation_expand(abbreviation):\n",
    "    \"\"\"\n",
    "    Find abbreviations using WordNet.\n",
    "\n",
    "    Args:\n",
    "    - abbreviation (str): The abbreviation to search for in WordNet.\n",
    "\n",
    "    Returns:\n",
    "    - res (list): A list of strings that represent possible meanings of the word.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    synsets = wn.synsets(abbreviation)\n",
    "    for synset in synsets:\n",
    "        ext_abb = str(synset)[8:-7].replace('_',' ')\n",
    "        res.append(ext_abb)\n",
    "    return res\n",
    "\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from emoji import emoji_list, is_emoji, demojize\n",
    "    \n",
    "    def detect_url(doc):\n",
    "        pattern = r'(?:https?\\:)?(?:\\/\\/)?(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z0-9]+(?:\\/[a-zA-z0-9]+)*'\n",
    "        return re.match(pattern, doc)\n",
    "    \n",
    "    punctuation_pattern = r'[!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]'\n",
    "    filtered_tokens = []\n",
    "    tokens = word_tokenize(doc)\n",
    "\n",
    "    for token in tokens:\n",
    "        # URL handling\n",
    "        if detect_url(token) is not None:\n",
    "            filtered_tokens.append(\"Link\")\n",
    "        # Emoji Handling\n",
    "        elif emoji_list(token):\n",
    "            normalized_emoji = demojize(token).replace(':', ' ').strip()\n",
    "            filtered_tokens.append(normalized_emoji)\n",
    "        else: \n",
    "            # If token matches a punctuation mark; ignore it\n",
    "            if re.match(punctuation_pattern, token) is None:\n",
    "                filtered_tokens.append(token) \n",
    "    return filtered_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining `TfidfVectorizer` model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Vectorizer \n",
    "vectorizer = TfidfVectorizer(      \n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train `TfidfVectorizer` model on dataset and queries\n",
    "Then storing `tfidf_matrix` and the `vectorizer` instance as `pickle` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         queries[query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train model \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# with open('feature_names.txt', 'w', encoding='utf-8') as fff:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     for feat_name in vectorizer.get_feature_names_out():\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#         fff.writelines(feat_name + ' ')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#     dill.dump(vectorizer, bf)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# bf.close()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 99\u001b[0m, in \u001b[0;36mcustom_tokenizer\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     97\u001b[0m     filtered_tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Emoji Handling\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43memoji_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    100\u001b[0m     normalized_emoji \u001b[38;5;241m=\u001b[39m demojize(token)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    101\u001b[0m     filtered_tokens\u001b[38;5;241m.\u001b[39mappend(normalized_emoji)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\emoji\\core.py:283\u001b[0m, in \u001b[0;36memoji_list\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m    277\u001b[0m         matches \u001b[38;5;241m=\u001b[39m filter_tokens(\n\u001b[0;32m    278\u001b[0m             matches, emoji_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, join_emoji\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(handle(m\u001b[38;5;241m.\u001b[39mvalue)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    280\u001b[0m         m\u001b[38;5;241m.\u001b[39mvalue, EmojiMatch) \u001b[38;5;28;01melse\u001b[39;00m m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches)\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memoji_list\u001b[39m(string):\n\u001b[0;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    Returns the location and emoji in list of dict format.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m        >>> emoji.emoji_list(\"Hi, I am fine. ðŸ˜\")\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m        [{'match_start': 15, 'match_end': 16, 'emoji': 'ðŸ˜'}]\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_start\u001b[39m\u001b[38;5;124m'\u001b[39m: m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mstart,\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_end\u001b[39m\u001b[38;5;124m'\u001b[39m: m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mend,\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memoji\u001b[39m\u001b[38;5;124m'\u001b[39m: m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39memoji,\n\u001b[0;32m    294\u001b[0m     } \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tokenize(string, keep_zwj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m\u001b[38;5;241m.\u001b[39mvalue, EmojiMatch)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Query looks like:\n",
    "    \"query\": {\n",
    "        \"_id\": string, // Can be mapped to numbers\n",
    "        \"text\": string,\n",
    "        \"metadata\": {\n",
    "            \"description\": string,\n",
    "            \"narrative\": string,\n",
    "        },\n",
    "    }\n",
    "\"\"\"\n",
    "queries = dict()\n",
    "with open('../webis-touche2020/queries.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for cnt, line in enumerate(f):\n",
    "        query = json.loads(line)\n",
    "        query_metadata = query['metadata']['narrative']\n",
    "        queries[query['_id']] = query['text'] \n",
    "\n",
    "# Train model \n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# with open('feature_names.txt', 'w', encoding='utf-8') as fff:\n",
    "#     for feat_name in vectorizer.get_feature_names_out():\n",
    "#         fff.writelines(feat_name + ' ')\n",
    "\n",
    "# Writing tfidf matrix to a compressed binary file \n",
    "# with open('matrix.pkl', 'wb') as mf:\n",
    "#     dill.dump(tfidf_matrix, mf)\n",
    "# mf.close()\n",
    "\n",
    "# # Writing vectorizer 'model' to a compressed binary file\n",
    "# with open('model.pkl', 'wb') as bf:\n",
    "#     dill.dump(vectorizer, bf)\n",
    "# bf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read `vectorizer` and `tfidf_matrix` from `pickle` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tfidf_matrix():\n",
    "    with open('matrix.pkl', 'rb') as mf:\n",
    "        tfidf_matrix = dill.load(mf)\n",
    "    mf.close()\n",
    "    return tfidf_matrix\n",
    "\n",
    "def get_vectorizer_instance() -> TfidfVectorizer:\n",
    "    with open('model.pkl', 'rb') as mf:\n",
    "        vectorizer = dill.load(mf)\n",
    "    mf.close()\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# tfidf_matrix = get_tfidf_matrix()\n",
    "# vectorizer = get_vectorizer_instance()\n",
    "\n",
    "def find_top_k_results(user_query, tfidf_matrix, docs, k=10): \n",
    "    # Transform user query into vector\n",
    "    user_query_vector = vectorizer.transform([user_query])\n",
    "\n",
    "    # Calculating cosine similarity between user query and stored data\n",
    "    similarites = cosine_similarity(user_query_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get indecies of top K elements\n",
    "    top_indices = np.argpartition(similarites, -k, axis=None)[-k:]\n",
    "\n",
    "    # Sort top K results by similarity score\n",
    "    top_indices_sorted = top_indices[np.argsort(similarites.ravel()[top_indices])]\n",
    "\n",
    "    # Get top K results\n",
    "    top_results = [(docs[i], keys[i], similarites[0, i]) for i in top_indices_sorted]\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "\n",
    "user_query = \"Should teachers get tenure?\"\n",
    "k = 10\n",
    "top_results = find_top_k_results(user_query, tfidf_matrix, docs, k)\n",
    "\n",
    "\n",
    "# Print\n",
    "with open('results.txt', 'w', encoding='utf-8') as rf:\n",
    "    rf.writelines\n",
    "    for doc, doc_id, similarity_score in top_results:\n",
    "        rf.writelines(f\"Doc Id: {doc_id}\\n\")\n",
    "        # rf.writelines(f\"Document: {doc}\\n\")\n",
    "        # rf.writelines(f\"Cosine Similarity Score: {similarity_score}\\n\")\n",
    "        # rf.writelines('--------------------------------------\\n')\n",
    "rf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "#### Calculating *precision* score and *recall* score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Precision: 0.7,  Recall: 0.1590909090909091, AP: 0.9129251700680271\n",
      "Mean Average Precision (mAP): 0.018631125919755655\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "qrels_file = '../webis-touche2020/qrels/test.tsv'\n",
    "\n",
    "def create_qrels_inverted(qrels_file):\n",
    "    inverted_index = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        next(f) # Skip first line of the file\n",
    "        for line in f:\n",
    "            query_id, document_id, score = line.strip().split('\\t')\n",
    "            score = int(score)\n",
    "            query_id = int(query_id)\n",
    "            if query_id not in inverted_index:\n",
    "                inverted_index[query_id] = []\n",
    "            inverted_index[query_id].append((document_id, score))\n",
    "                    \n",
    "    # Sort index in descending order\n",
    "    for query_id, doc_scores in inverted_index.items():\n",
    "        inverted_index[query_id] = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return inverted_index\n",
    "\n",
    "def get_relevant_docs_for_query_i(query_id, k=10):\n",
    "    relevant_docs = set()\n",
    "    # for query_id, doc_scores in zip(qrels.keys(), qrels.values()):\n",
    "    for doc_id, score in qrels[query_id]:\n",
    "        relevant_docs.add(doc_id)\n",
    "    return relevant_docs\n",
    "\n",
    "def calculate_average_precision(relevant_docs, retrieved_docs, k=10): \n",
    "    precision_sum = 0\n",
    "    true_positives_at_k = 0\n",
    "    for i, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if retrieved_doc in relevant_docs:\n",
    "            true_positives_at_k += 1\n",
    "            precision_sum += true_positives_at_k / i\n",
    "    return precision_sum / true_positives_at_k if true_positives_at_k > 0 else 0\n",
    "        \n",
    "def evaluate(top_results: List, query_id: str, k):\n",
    "    relevant_docs = get_relevant_docs_for_query_i(query_id)\n",
    "    retrieved_docs = set(result[1] for result in top_results)\n",
    "   \n",
    "    true_positives = retrieved_docs.intersection(relevant_docs)\n",
    "    \n",
    "    # Precision\n",
    "    precision = len(true_positives) / k if k > 0 else 0\n",
    "    # Recall \n",
    "    recall = len(true_positives)/ len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "    # Average Precision\n",
    "    ap = calculate_average_precision(relevant_docs, retrieved_docs)\n",
    "    \n",
    "    return precision, recall, ap\n",
    "\n",
    "qrels = create_qrels_inverted(qrels_file)\n",
    "\n",
    "aps = []\n",
    "with open('evaluation.tsv', 'w', newline='', encoding='utf-8') as ef:  \n",
    "    writer = csv.writer(ef, delimiter='\\t')  \n",
    "    writer.writerow(['Query-id', 'Precision', 'Recall', 'AP'])\n",
    "    for query_id in queries.keys():\n",
    "        p, r, ap = evaluate(top_results, query_id, k)\n",
    "        aps.append(ap)\n",
    "        writer.writerow([query_id, p, r, ap])\n",
    "        if p != 0:\n",
    "            print(f\"Query {query_id}: Precision: {p},  Recall: {r}, AP: {ap}\")\n",
    "ef.close()\n",
    "\n",
    "\n",
    "mAP = sum(aps) / len(aps)\n",
    "print(f\"Mean Average Precision (mAP): {mAP}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
